{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The download page for the IPEDS data center is accessed via a javascript form, but I've stored a current (fall 2015) copy. This notebook will download all of the zipped data files and their dictionaries, and put them in the correct directories. The full collection of files is more than a gigabyte, zipped, so I'm not storing it in git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the page and grab the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open('../data_archives/IPEDS Data Center.html'), 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = soup.find_all('tr', \"idc_gridviewrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datalinks = []\n",
    "dictionarylinks = []\n",
    "\n",
    "csvfile = re.compile(\"^((?!STATA|SPSS|SAS|Dictionary).)*$\")\n",
    "\n",
    "for row in rows:\n",
    "    datalinks.append(row.find(\"a\", text=csvfile).attrs['href'])\n",
    "    dictionarylinks.append(row.find(\"a\", text=\"Dictionary\").attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(datalinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datalinks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form of the link shows that we will have to prepend the bulk of the web address.\n",
    "\n",
    "\n",
    "## Download and unpack the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_and_unpack(link, directory, redownload=False, verbose=False):\n",
    "    \"\"\"Download an IPEDS data file and unzip it.\n",
    "    \n",
    "    Could use some error handling.\n",
    "    \"\"\"\n",
    "    os.chdir(\"../\" + directory)\n",
    "\n",
    "    zipfilename = link.split(\"/\")[-1]\n",
    "    filebase = zipfilename.split(\".\")[0]\n",
    "    zipfilepath = \"../data_archives/\" + zipfilename\n",
    "\n",
    "    linkbase = \"https://nces.ed.gov/ipeds/datacenter/\"\n",
    "    if redownload or not os.path.exists(zipfilepath):    \n",
    "        try:\n",
    "            r = requests.get(linkbase + link)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(err)\n",
    "            \n",
    "        chunk_size = 1024\n",
    "        with open(zipfilepath, 'wb') as fd:\n",
    "            for chunk in r.iter_content(chunk_size):\n",
    "                fd.write(chunk)\n",
    "        if verbose:\n",
    "            print(\"Downloaded {}\".format(zipfilepath))\n",
    "            \n",
    "        csvzip = zipfile.ZipFile(zipfilepath)\n",
    "        csvzip.extractall()\n",
    "        if verbose:\n",
    "            print(\"unpacked\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"file already downloaded: {}\".format(zipfilepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_and_unpack(dictionarylinks[-3], \"dictionaries\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_and_unpack(datalinks[1], \"data\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some throttling to be kind to their webserver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for link in datalinks:\n",
    "    download_and_unpack(link, 'data', verbose=True)\n",
    "    #time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for link in dictionarylinks:\n",
    "    download_and_unpack(link, 'dictionaries', verbose=True)\n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
